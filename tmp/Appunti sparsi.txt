NOTA 1
Ha senso mettere i dati del 2017 nel training + validation set (da trattare con k-fold cross validation) e i dati del 2018 nel testing set.

NOTA 2
Ha senso normalizzare i valori di tutte le feature in modo tale che siano dello stesso ordine di grandezza.

NOTA 3
LDA == GDA con matrici delle covarianze tutte uguali; QDA == GDA con matrici delle covarianze diverse.

NOTA 4
Il paper fa riferimento alla possibilità di avere dalle 0 alle 3 persone nella stanza, anche se le scrivanie in realtà sono 4. 

NOTA 5
S6_PIR e S7_PIR sono due sensori che rilevano la presenza di movimento mediante la presenza di raggi infrarossi; danno un output pari a 1 se c'è movimento, 0 altrimenti.

NOTA 6
Il paper NON tratta data e ora come feature, ma potrebbero comunque essere prese in considerazione.

NOTA 7
Le feature S5_CO2, S5_CO2_slope (dove slope == pendenza) sembrerebbero essere i due indicatori migliori per stimare il numero di persone nella stanza. Tuttavia, quando il numero delle persone varia, i valori di queste feature impiegano diversi minuti per arrivare a steady-state.

NOTA 8
Anche le feature sulla luminosità della stanza sono molto d'aiuto: nel paper si è osservato che molto probabilmente le persone accendono la luce di una scrivania quando arrivano e la spengono solo quando se ne vanno (NB: occhio alle persone che escono dalla stanza dimenticandosi la luce accesa).

NOTA 9
Algoritmi di machine learning utilizzati e citati nel paper: LDA, QDA, RF, SVM; PDA (praticamente si è sfruttata solo la classificazione: ha senso ricorrere anche alla regressione).

NOTA 10
Stando al paper, SVM con kernel gaussiano si comporta tipicamente molto meglio di SVM lineare (ma la differenza non è molta se si considerano tutte le feature). Inoltre, SVM con kernel gaussiano sembra essere il modello che si comporta meglio quando viene applicato PCA.

NOTA 11
Metriche citate nel paper per valutare le performance dei classificatori: F1-score, confusion matrix, accuracy.

NOTA 12
Libreria di machine learning di Python citata nel paper: Scikit-learn.



CLASSIFICAZIONE
a) GDA --> sia LDA che QDA
b) Random Forest
c) SVM --> sia lineare (kernel = prodotto scalare phi(x_1)*phi(x_2)), sia con kernel gaussiano, sia con kernel logistico, sia con kernel softmax
d) Regressione softmax
e) Classificazione fatta con l'appoggio della regressione (funzione discriminante)
f) Adaboost
g) Rete neurale (con phi() == softmax e h() == RELU, sigmoide, tanh)

REGRESSIONE
a) Regressione lineare
b) MAP
c) Approccio totalmente bayesiano
d) Nadaraya-Watson
e) Locally Weighted Regression
f) Gradient Boosting
g) Rete neurale (con phi() == combinazione lineare e h() == RELU, sigmoide, tanh)

-> Sia nella classificazione che nella regressione si possono effettuare gli esperimenti sia con PCA che senza PCA.